"""
Context Compressor — keeps conversation history from bankrupting you.

Problem: an interview at turn 50 sends all 50 turns to Claude every time.
Turn 50 costs 50x more in input tokens than turn 1. History grows linearly.
Prompt caching helps but only if the messages haven't changed — they have.

Solution: hierarchical compression.
  - Keep system prompt (cached — 10% cost)
  - Keep last N turns verbatim (recency matters for coherence)
  - Replace everything before that with a structured running summary
  - Summary generated by Haiku ($0.25/1M) — fast, cheap, good enough

Result: O(1) token cost per turn instead of O(n). Capped at ~2000 tokens
regardless of how long the conversation runs.

Usage:
    from context_compressor import ContextCompressor
    compressor = ContextCompressor()

    # In your interview loop:
    messages = compressor.compress(messages, max_recent=4)

    # Or full pipeline — summarise + trim in one call:
    messages = await compressor.compress_async(messages, max_recent=4)

Token budget:
    Summary:       ~300-500 tokens (structured facts extracted by Haiku)
    Recent turns:  ~400-800 tokens (last 4 turns verbatim)
    Total history: ~700-1300 tokens regardless of interview length

    Without compression, turn 50 of an interview = ~8,000-15,000 tokens.
    Savings per turn: 85-95% on history tokens.

Integration points:
    - claude_client.py: call before messages.create()
    - token_proxy.py: call at proxy level (works for all tools)
    - Standalone: compress any conversation history anywhere

Status:
    DONE — core compression logic, structured summary format
    DONE — sync and async versions
    DONE — token estimation, compression ratio logging
    UNFINISHED — LLMLingua integration (see note below). Currently uses
                 Haiku summarisation which is simpler and cheaper for this use
                 case. LLMLingua is better for prompt compression (static text),
                 Haiku summarisation is better for conversation compression
                 (dynamic, structured extraction).
"""

import os
import sys
import json
from pathlib import Path
from typing import List, Dict, Optional

# Load API keys via secrets manager (keys.env → 1Password when configured)
sys.path.insert(0, str(Path.home() / ".amplified"))
from amplified_secrets import secrets  # noqa: F401 — populates os.environ on import

import anthropic

# ---- Config ----------------------------------------------------------------

HAIKU = "claude-haiku-4-5-20251001"
DEFAULT_MAX_RECENT = 4       # Keep this many turns verbatim
DEFAULT_COMPRESS_THRESHOLD = 8  # Start compressing after this many turns
SUMMARY_MAX_TOKENS = 600     # Haiku's summary budget


# ---- Token estimation (rough — 1 token ≈ 4 chars) -------------------------

def estimate_tokens(messages: List[Dict]) -> int:
    total = 0
    for msg in messages:
        content = msg.get("content", "")
        if isinstance(content, list):
            content = " ".join(b.get("text", "") for b in content if isinstance(b, dict))
        total += len(str(content)) // 4
    return total


# ---- Summary prompt --------------------------------------------------------

SUMMARY_SYSTEM = """You extract structured facts from conversation history.
Output JSON only — no explanation, no preamble."""

SUMMARY_PROMPT = """Extract all information from this conversation history into a structured summary.

Format:
{
  "context": "1-2 sentence description of what this conversation is about",
  "established_facts": ["fact 1", "fact 2", ...],
  "decisions_made": ["decision 1", ...],
  "open_questions": ["question still to answer", ...],
  "person_profile": {
    "name": "if known",
    "role": "if known",
    "key_details": ["important personal/professional details"]
  },
  "topics_covered": ["topic 1", "topic 2", ...]
}

Be complete. Every important fact must appear. These facts replace the original messages.

CONVERSATION HISTORY:
{history}"""


def _messages_to_text(messages: List[Dict]) -> str:
    lines = []
    for msg in messages:
        role = msg.get("role", "unknown").upper()
        content = msg.get("content", "")
        if isinstance(content, list):
            content = " ".join(b.get("text", "") for b in content if isinstance(b, dict))
        lines.append(f"{role}: {content}")
    return "\n\n".join(lines)


def _summary_to_message(summary: dict) -> Dict:
    """Convert structured summary dict into a single synthetic user message."""
    lines = ["[CONVERSATION SUMMARY — earlier turns compressed]", ""]

    if summary.get("context"):
        lines.append(f"Context: {summary['context']}")
        lines.append("")

    if summary.get("person_profile"):
        p = summary["person_profile"]
        profile_parts = []
        if p.get("name"):
            profile_parts.append(f"Name: {p['name']}")
        if p.get("role"):
            profile_parts.append(f"Role: {p['role']}")
        if p.get("key_details"):
            profile_parts.extend(p["key_details"])
        if profile_parts:
            lines.append("Person: " + " | ".join(profile_parts))
            lines.append("")

    if summary.get("established_facts"):
        lines.append("Established facts:")
        for f in summary["established_facts"]:
            lines.append(f"  • {f}")
        lines.append("")

    if summary.get("decisions_made"):
        lines.append("Decisions made:")
        for d in summary["decisions_made"]:
            lines.append(f"  • {d}")
        lines.append("")

    if summary.get("topics_covered"):
        lines.append(f"Topics covered: {', '.join(summary['topics_covered'])}")
        lines.append("")

    if summary.get("open_questions"):
        lines.append("Still to address:")
        for q in summary["open_questions"]:
            lines.append(f"  • {q}")

    return {"role": "user", "content": "\n".join(lines)}


# ---- Compressor ------------------------------------------------------------

class ContextCompressor:
    """
    Compresses conversation history using hierarchical summarisation.

    The first call to compress() that exceeds the threshold will call Haiku
    to generate a structured summary of the older messages. Subsequent calls
    replace/update that summary rather than growing it.
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        max_recent: int = DEFAULT_MAX_RECENT,
        compress_threshold: int = DEFAULT_COMPRESS_THRESHOLD,
    ):
        self.client = anthropic.Anthropic(api_key=api_key or os.getenv("ANTHROPIC_API_KEY"))
        self.max_recent = max_recent
        self.compress_threshold = compress_threshold
        self._summary_cache: Optional[str] = None  # raw JSON string

    def needs_compression(self, messages: List[Dict]) -> bool:
        # Already has a summary block? Check if recent turns have grown enough to re-compress.
        has_summary = any(
            "[CONVERSATION SUMMARY" in str(m.get("content", ""))
            for m in messages
        )
        if has_summary:
            # Re-compress if we've added more than max_recent turns since last summary
            non_summary = [m for m in messages if "[CONVERSATION SUMMARY" not in str(m.get("content", ""))]
            return len(non_summary) > self.max_recent
        return len(messages) > self.compress_threshold

    def _generate_summary(self, messages_to_summarise: List[Dict]) -> Optional[dict]:
        """Call Haiku to extract structured facts from old messages."""
        history_text = _messages_to_text(messages_to_summarise)
        prompt = SUMMARY_PROMPT.format(history=history_text)

        try:
            response = self.client.messages.create(
                model=HAIKU,
                max_tokens=SUMMARY_MAX_TOKENS,
                temperature=0.1,
                system=SUMMARY_SYSTEM,
                messages=[{"role": "user", "content": prompt}],
            )
            raw = response.content[0].text.strip()
            # Strip markdown code fences if present
            if raw.startswith("```"):
                raw = raw.split("```")[1]
                if raw.startswith("json"):
                    raw = raw[4:]
            return json.loads(raw)
        except Exception as e:
            print(f"[ContextCompressor] Summary generation failed: {e}")
            return None

    def compress(self, messages: List[Dict]) -> List[Dict]:
        """
        Synchronous compression. Returns compressed message list.

        Structure of result:
          [summary_message, <last max_recent turns>]

        where summary_message is a synthetic user message containing
        structured facts about everything that came before.
        """
        if not self.needs_compression(messages):
            return messages

        before_tokens = estimate_tokens(messages)

        # Separate existing summary (if any) from real messages
        existing_summary_msgs = [m for m in messages if "[CONVERSATION SUMMARY" in str(m.get("content", ""))]
        real_msgs = [m for m in messages if "[CONVERSATION SUMMARY" not in str(m.get("content", ""))]

        # Split real messages into old (to summarise) and recent (to keep verbatim)
        recent = real_msgs[-self.max_recent:] if len(real_msgs) > self.max_recent else real_msgs
        old = real_msgs[:-self.max_recent] if len(real_msgs) > self.max_recent else []

        if not old:
            return messages

        # If there's a previous summary, include its text in what we're re-summarising
        to_summarise = []
        if existing_summary_msgs:
            # Convert previous summary back into context for re-summarisation
            for sm in existing_summary_msgs:
                to_summarise.append({"role": "assistant", "content": f"[Previous summary context]: {sm['content']}"})
        to_summarise.extend(old)

        summary_dict = self._generate_summary(to_summarise)
        if not summary_dict:
            # Failed — return original rather than losing data
            return messages

        summary_msg = _summary_to_message(summary_dict)
        result = [summary_msg] + recent

        after_tokens = estimate_tokens(result)
        ratio = (1 - after_tokens / before_tokens) * 100 if before_tokens > 0 else 0
        print(
            f"[ContextCompressor] {len(messages)} msgs → {len(result)} msgs | "
            f"~{before_tokens} → ~{after_tokens} tokens | "
            f"{ratio:.0f}% reduction"
        )

        return result

    async def compress_async(self, messages: List[Dict]) -> List[Dict]:
        """Async wrapper — runs compression in a thread pool."""
        import asyncio
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.compress, messages)


# ---- Standalone usage ------------------------------------------------------

def compress_messages(
    messages: List[Dict],
    api_key: Optional[str] = None,
    max_recent: int = DEFAULT_MAX_RECENT,
) -> List[Dict]:
    """
    One-shot compression. Stateless convenience function.

    compressor = None  (creates a fresh one)
    Good for: fire-and-forget, one-off calls, proxy integration.
    """
    c = ContextCompressor(api_key=api_key, max_recent=max_recent)
    return c.compress(messages)
